A few thoughts and observations regarding scaling and perfomance:
 

 - [ ] For generating the short URLs, 7 characters are used from a pool of 62 options (lower/upper ascii and digits). This means that 62^7 unique short URLs are possible, not counting for reusability of expired URLs.
 - [ ] Since colIisions are not very likely to occur often, I chose to randomly generate a short URL per request and try to create the entry. If the URL already exists, we just try again with a new one.
 This is guarenteed to work (provided we keep trying) even when scaling the app since atomic transactions are provided by the DB.
 Alternatively,  it is possible to use a global running counter which will be incremented on every request,  translated to url-safe base 64 (or base 62). This would ensure that no short URL is ever re-used, but introcudes a much bigger problem - synchronizing the counter between all instances of the app. Therefore I decided against it.
 - [ ] Obviously, the system should be able to scale well. I've been working under the assumption that the system will be much more read-heavy than write-heavy. Therefore, optimizing for read efficiency is the most important factor in terms of performance. Since we are dealing with a potentially huge number of records, there are three key insights:
	- We can only ever use the PK (which is indexed) for fetching records.
	- Since the data is potentially ephemral, it might be worthwhile to "clean" old records which are no longer used to keep the DB as lean as possible. I've addressed this issue by creating a django command (`remove_expired_urls`) which is designed to be run periodically, ideally during low-traffic hours. The script will remove non-relevant records from the table, potentially copying them to another storage service for future analytics. Keeping the DB size in check will help ensure fast reads.
	Another benefit to keeping the DB small is that it avoids (or at least postpones) the need for sharding. Using a single DB for every instance makes life easier since we have guaranteed atomicty on every transaction.
	- Incrementing the hit counter ***directly*** on every request will needlessly access the DB and slow down the response. It is also not necessary to increment instantly on every request, since the hit counter will probably not be checked very often.
	Therefore, I think that an interesting design choice would be to use a cache service (`redis` comes to mind) for the increment process: on every request, the server will insert a ***unique*** entry to the cache which corresponds to the request. For example, it could be something like `short_url_{uuid}` (alternatively, it can just increment the value of `short_url` in an existing hash - although this implementation feels less safe when scaling to multiple app servers). Then, another service could be responsible for periodically going over the cache and incremeting the hit counters accordingly. This means that on each write we only need to access the cache which speeds up the response.
